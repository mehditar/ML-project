# independent-project
## <font color='blue'>**Analysis of the severity of car accidents in U.S **</font>
In this project, I analysed a dataset of  car crashes happened in the U.S. between 2016 to 2020. This dataset contains 1.2 million accidents where for each information about the time of the accident, exact location of the accident, the weather information at the time of the accident and severity of the accident exists in the dataset.
  To do the analysis, in the first step, i did some data cleaning. Feature extraction was necessary and was the second step of this analysis. Then the effects of each feature on the severity of the accidents was analysed. And finally using deep learning models, a model for predicting the severity of car accidents just using the provided features was developed and tested.
  
### <font color='green'>**Dependencies:**</font>
1. Numpy 
2. Pandas 
3. Sikit-learn
4. Keras(Tensorflow - backend)
5. Tensorflow 2.0
6. Matplotlib
7. Seaborn

### <font color='green'>**Datasets:**</font>
* The main dataset was downloaded from kaggle at [here](https://www.kaggle.com/sobhanmoosavi/us-accidents) : the size of this CSV format dataset is  570 M and it contains  dataframe with 1.2 million rows and 47 collumns.

* A word representation dataset, GloVe from Stanford university at [here](https://nlp.stanford.edu/projects/glove/).


